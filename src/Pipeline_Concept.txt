prepare.py -> takes the raw datasets, maps column names, and splits into train and validate.
              might be worth normalizing X and y column names??
              

utils -> featurize.py -> generate_morgan_fingerprint()

utils -> loader.py -> load data to train_test, pickle models, etc..

train.py -> takes in datasets, creates features, trains the models, records training time, and pickles models,
            Do we want to also split by clustering vs random here?

evaluate.py -> outputs a csv with all model results 

process_pipeline.py -> basically a main script that trains the process_pipeline
                       arguments (dataset, model, hyperparams, (option to provide comma separated list))

predict.py ?? -> 