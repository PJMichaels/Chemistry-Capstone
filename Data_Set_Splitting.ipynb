{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea6c349b-0ac0-4201-b909-6ed67a13d476",
   "metadata": {},
   "source": [
    "# Notebook to perform data splitting for train/test & val\n",
    "We plan to use a single train/test data set as our models will use cross validation in order to train, which should help give some idea of how well the model generalizes initially. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "86bd8e1d-d2ee-41c9-9059-d5b1e5222fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to do - get this to loop over all the datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c231282e-8b3b-407f-9107-576a8aa4ba04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from rdkit.Chem import AllChem\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "tqdm.pandas()\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "efc4b94f-e408-40fc-bfbf-7b76bd48a31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path='data_cleaned'\n",
    "out_path='data_split_cleaned'\n",
    "\n",
    "files=os.listdir(path)\n",
    "data_map={\n",
    "    'HIV.csv': {'target':'HIV_active','structure':'smiles'},\n",
    "    'bace.csv':{'target':'active','structure':'mol'},\n",
    "    'tox21.csv':{'target':'NR-AhR','structure':'smiles'},\n",
    "    'clintox.csv':{'target':'CT_TOX','structure':'smiles'},\n",
    "    'sol_del.csv':{'target':'binned_sol','structure':'smiles'},\n",
    "    'deepchem_Lipophilicity.csv':{'target':'drug_like','structure':'smiles'}   \n",
    "}\n",
    "\n",
    "validation_size=0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b5ae80c0-f570-4a85-8f95-5473c7631e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test file\n",
    "files = os.listdir(path)\n",
    "\n",
    "df=pd.read_csv(os.path.join(path,'bace.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d77be196-af2a-43b5-a247-75c7e15354a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fingerprint(smiles,radius,bits):\n",
    "    try:\n",
    "        mol=Chem.MolFromSmiles(smiles)\n",
    "        fp=AllChem.GetMorganFingerprintAsBitVect(mol,radius,bits)\n",
    "        return(np.array(fp))\n",
    "    except:\n",
    "        print(f'{smiles} failed in RDkit')\n",
    "        return (np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "83476082-c45d-441c-9524-180be7668ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the morgan finger prints and drop any rows that don't convert to a mol, or those that don't have a target value\n",
    "radius=2\n",
    "bits=1024\n",
    "df['fp'] = df['mol'].apply(lambda x: generate_fingerprint(x,radius,bits))\n",
    "df.dropna(subset=['fp','active'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a905c86f-72e9-4568-aa93-70cedf3e6b33",
   "metadata": {},
   "source": [
    "# Random split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "df6a4390-5526-464a-a7b5-5baa080b86a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_validate = train_test_split(df,test_size=validation_size,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "fd888dae-69c3-4ebd-a5c7-2feafef83333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1286, 598)\n",
      "1    856\n",
      "0    430\n",
      "Name: active, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_train.shape)\n",
    "print(df_train['active'].value_counts())\n",
    "#df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "44a8498d-f265-4e14-bcab-d84ad0a71a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(227, 598)\n",
      "1    156\n",
      "0     71\n",
      "Name: active, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_validate.shape)\n",
    "print(df_validate['active'].value_counts())\n",
    "#df_validate.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "274cf5e2-e1d5-4f1b-b269-d1b1c229d782",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset='bace'\n",
    "df_train.to_csv(os.path.join(out_path,f'{dataset}_train.csv'))\n",
    "df_validate.to_csv(os.path.join(out_path,f'{dataset}_validate.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72931ed-6069-4aef-bb8d-ba23bd1cb355",
   "metadata": {},
   "source": [
    "# Clustered Split\n",
    "To make the split more realistic and understand how well the model will generalize to new data, let's also look at a split based on the data clusters. The goal is to minimize overlap in terms of chemical structure between the two data sets. This will effectively make it much harder for a model that just memorizes certain aspects of the potent scaffolds for instance.<br>\n",
    "This code was inspired by the chemprop scaffold based split: https://chemprop.readthedocs.io/en/latest/_modules/chemprop/data/scaffold.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e7ab2371-8f77-4071-a4bd-b18592e20a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "clusters=int(df.shape[0]/30)\n",
    "kmeans = MiniBatchKMeans(n_clusters=clusters,random_state=0,batch_size=100).fit(df['fp'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "11be81bc-300d-4f94-bda6-f890c7c6df6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cluster']=kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "325fc925-9785-423e-9928-801fd3820154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9     157\n",
       "25     82\n",
       "7      79\n",
       "8      75\n",
       "12     67\n",
       "36     67\n",
       "39     62\n",
       "3      57\n",
       "27     54\n",
       "34     47\n",
       "35     45\n",
       "18     37\n",
       "16     36\n",
       "49     36\n",
       "48     35\n",
       "10     34\n",
       "5      33\n",
       "30     33\n",
       "33     33\n",
       "17     30\n",
       "19     29\n",
       "6      28\n",
       "32     28\n",
       "22     27\n",
       "38     25\n",
       "26     24\n",
       "13     21\n",
       "40     20\n",
       "14     18\n",
       "28     18\n",
       "46     16\n",
       "43     15\n",
       "42     15\n",
       "23     15\n",
       "2      14\n",
       "20     12\n",
       "15     11\n",
       "37     10\n",
       "4      10\n",
       "45      9\n",
       "31      8\n",
       "44      8\n",
       "1       8\n",
       "47      7\n",
       "0       4\n",
       "29      4\n",
       "21      4\n",
       "24      2\n",
       "41      2\n",
       "11      2\n",
       "Name: cluster, dtype: int64"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "8c38694c-c700-4d0c-a7fe-330a90e00dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add these clusters to two groups, train and val:\n",
    "val_size=validation_size*len(df)\n",
    "df_cluster_train=pd.DataFrame()\n",
    "df_cluster_val=pd.DataFrame()\n",
    "for group, dataframe in df.groupby('cluster'):\n",
    "    if dataframe.shape[0] > val_size / 2:\n",
    "        df_cluster_train=pd.concat([df_cluster_train,dataframe])\n",
    "    elif len(df_cluster_val)+len(dataframe) <= val_size:\n",
    "        df_cluster_val=pd.concat([df_cluster_val,dataframe])\n",
    "    else:\n",
    "        df_cluster_train=pd.concat([df_cluster_train,dataframe])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "09622811-fbae-45e7-82ba-f3d80a313d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1287, 599)\n",
      "1    845\n",
      "0    442\n",
      "Name: active, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_cluster_train.shape)\n",
    "print(df_cluster_train['active'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "7193e681-1495-4a48-b603-132f894aceb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(226, 599)\n",
      "1    167\n",
      "0     59\n",
      "Name: active, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_cluster_val.shape)\n",
    "print(df_cluster_val['active'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "01a59cc1-eba9-4b30-950b-c66b649d79a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class balance is a little different between the two, however this should outweigh issues arising from\n",
    "# the scaffolds being shared between both. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "26bb91aa-8f0a-417b-aeed-72c5409f1c60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6565656565656566"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "845/(845+442)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "93daa5cf-7cee-42ad-bdce-2d6f68559126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7389380530973452"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "167/(167+59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "543b9c61-1b5f-4cc2-9377-4059f8ffcd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset='bace'\n",
    "df_cluster_train.to_csv(os.path.join(out_path_cluster,f'{dataset}_cluster_train.csv'))\n",
    "df_cluster_val.to_csv(os.path.join(out_path_cluster,f'{dataset}_cluster_validate.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60e123c-0e7d-4a8e-a0f3-4933b17fb868",
   "metadata": {},
   "source": [
    "# Apply this splitting to all of the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "d2f2d2de-8d26-49d6-b4ac-595ca513e4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deepchem_Lipophilicity.csv\n",
      "sol_del.csv\n",
      "HIV.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[15:03:54] WARNING: not removing hydrogen atom without neighbors\n",
      "[15:03:54] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clintox.csv\n",
      "[NH4][Pt]([NH4])(Cl)Cl failed in RDkit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[15:04:40] Explicit valence for atom # 0 N, 5, is greater than permitted\n",
      "[15:04:40] Can't kekulize mol.  Unkekulized atoms: 9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c1ccc(cc1)n2c(=O)c(c(=O)n2c3ccccc3)CCS(=O)c4ccccc4 failed in RDkit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[15:04:41] Explicit valence for atom # 10 N, 4, is greater than permitted\n",
      "[15:04:41] Explicit valence for atom # 10 N, 4, is greater than permitted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cc1cc2c(cc1C)N3C=N2[Co+]456(N7=C8[C@H](C(C7=CC9=N4C(=C(C1=N5[C@@]([C@@H]2N6C(=C8C)[C@@]([C@H]2CC(=O)N)(CCC(=O)NC[C@H](OP(=O)(O[C@@H]2[C@H](O[C@H]3[C@@H]2O)CO)[O-])C)C)([C@@]([C@@H]1CCC(=O)N)(C)CC(=O)N)C)C)[C@@]([C@@H]9CCC(=O)N)(C)CC(=O)N)(C)C)CCC(=O)N)O failed in RDkit\n",
      "Cc1cc2c(cc1C)N3C=N2[Co]456(N7=C8[C@H](C(C7=CC9=N4C(=C(C1=N5[C@@]([C@@H]2N6C(=C8C)[C@@]([C@H]2CC(=O)N)(CCC(=O)NC[C@H](OP(=O)(O[C@@H]2[C@H](O[C@H]3[C@@H]2O)CO)O)C)C)([C@@]([C@@H]1CCC(=O)N)(C)CC(=O)N)C)C)[C@@]([C@@H]9CCC(=O)N)(C)CC(=O)N)(C)C)CCC(=O)N)C#N failed in RDkit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[15:04:41] Can't kekulize mol.  Unkekulized atoms: 4\n",
      "[15:04:41] Can't kekulize mol.  Unkekulized atoms: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CCCCc1c(=O)n(n(c1=O)c2ccc(cc2)O)c3ccccc3 failed in RDkit\n",
      "CCCCc1c(=O)n(n(c1=O)c2ccccc2)c3ccccc3 failed in RDkit\n",
      "bace.csv\n",
      "tox21.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[15:04:45] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    }
   ],
   "source": [
    "# setup loop to split datasets\n",
    "\n",
    "for file in files:\n",
    "    if file[-4:]=='.csv':\n",
    "        print(file)\n",
    "        dataset_name=file.replace('.csv','')\n",
    "        df=pd.read_csv(os.path.join(path,file))\n",
    "        # extract the dataset features:\n",
    "        target=data_map[file]['target']\n",
    "        smiles=data_map[file]['structure']\n",
    "\n",
    "        # add the features:\n",
    "        radius=2\n",
    "        bits=1024\n",
    "        df['fp'] = df[smiles].apply(lambda x: generate_fingerprint(x,radius,bits))\n",
    "        df.dropna(subset=['fp',target],inplace=True)\n",
    "        \n",
    "        #perform a random split:\n",
    "        df_train, df_validate = train_test_split(df,test_size=validation_size,random_state=0)\n",
    "        df_train.to_csv(os.path.join(out_path,f'{dataset_name}-random-train.csv'))\n",
    "        df_validate.to_csv(os.path.join(out_path,f'{dataset_name}-random-validate.csv'))\n",
    "        \n",
    "        # perform the clusterd split:\n",
    "        clusters=int(df.shape[0]/30) # generate a rough number of clusters\n",
    "        kmeans = MiniBatchKMeans(n_clusters=clusters,random_state=0,batch_size=100).fit(df['fp'].to_list())\n",
    "        df['cluster']=kmeans.labels_\n",
    "        # add these clusters to two groups, train and val:\n",
    "        val_size=validation_size*len(df)\n",
    "        df_cluster_train=pd.DataFrame()\n",
    "        df_cluster_val=pd.DataFrame()\n",
    "        for group, dataframe in df.groupby('cluster'):\n",
    "            if dataframe.shape[0] > val_size / 2:\n",
    "                df_cluster_train=pd.concat([df_cluster_train,dataframe])\n",
    "            elif len(df_cluster_val)+len(dataframe) <= val_size:\n",
    "                df_cluster_val=pd.concat([df_cluster_val,dataframe])\n",
    "            else:\n",
    "                df_cluster_train=pd.concat([df_cluster_train,dataframe])\n",
    "        # write the clustered sets to a file:\n",
    "        df_cluster_train.to_csv(os.path.join(out_path,f'{dataset_name}-cluster-train.csv'))\n",
    "        df_cluster_val.to_csv(os.path.join(out_path,f'{dataset_name}-cluster-validate.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586a4c9a-56c6-4e43-a1f5-dfb9584c281b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
